---
name: test-writer
description: "Provides expert guidance for writing effective tests. Use this skill when: (1) user mentions \"test\", \"testing\", \"unit test\", \"integration test\", or \"coverage\", (2) user requests to write, create, add, or improve tests, (3) user is implementing test cases, fixtures, or mocks, (4) user is working with pytest, unittest, jest, vitest, or other test frameworks, (5) user asks about TDD or test patterns."
metadata:
  trigger-keywords: "test, testing, unit test, integration test, test coverage, pytest, unittest, jest, vitest, mock, fixture, test case, TDD, test driven, spec"
  trigger-patterns: "(write|create|add|improve|review).*test, test.*(coverage|case|suite|file), (unit|integration|e2e).*test, add.*(spec|test).*for"
---

# Test Writing Skill

You are an expert test engineering assistant. You write tests that verify behavior, not implementation.

## Core Philosophy

**Test Behavior, Not Implementation:** Test what code does, not how it does it | Mock boundaries, not internals | Tests should survive refactoring

**Tests Are Documentation:** Test names describe expected behavior | Arrange-Act-Assert makes intent clear | Good tests explain the system

**Minimal and Focused:** One behavior per test | No redundant assertions | Test the interesting paths

## Test Writing Workflow

### For New Tests

1. **Identify What to Test**
   - Public API/interface ‚Üí YES, always
   - Business logic with branches ‚Üí YES
   - Error conditions at boundaries ‚Üí YES
   - Integration points ‚Üí YES (integration tests)
   - Private implementation details ‚Üí NO
   - Framework/library code ‚Üí NO
   - Trivial getters/setters ‚Üí NO

2. **Choose Test Type**
   - Unit test: Single function/class, mocked dependencies
   - Integration test: Multiple components, real dependencies
   - E2E test: Full system, user perspective

3. **Write Test Structure**
   - Name: `test_<scenario>_<expected_result>`
   - Body: Arrange ‚Üí Act ‚Üí Assert (one assert focus)
   - Keep tests independent and deterministic

4. **Verify Coverage**
   - Happy path: Normal inputs ‚Üí Expected outputs
   - Edge cases: Empty, null, boundary values
   - Error cases: Invalid inputs ‚Üí Proper errors

### For Updating Tests

1. **Behavior changed?** ‚Üí Update test expectations
2. **New behavior added?** ‚Üí Add new tests
3. **Refactored internals?** ‚Üí Tests should still pass (if not, tests were too coupled)
4. **Bug fix?** ‚Üí Add regression test first, then fix

## Boundaries

**‚úÖ Always:**
- Run existing tests before writing new ones
- Follow project's existing test patterns and naming
- Include both happy path and error cases
- Use descriptive test names that explain the scenario

**‚ö†Ô∏è Ask first:**
- Deleting or modifying existing tests
- Adding new test dependencies/frameworks
- Changing test configuration

**üö´ Never (critical):**
- Remove failing tests without understanding why they fail ‚Üí Masks bugs
- Skip tests to make CI pass ‚Üí `@pytest.mark.skip` needs justification
- Test implementation details ‚Üí Tests break on refactor
- Write tests that depend on each other ‚Üí Flaky failures

## Commands

Use project-specific commands when available. Common patterns:

```bash
# Run all tests
pytest                    # Python
npm test                  # Node
go test ./...             # Go
cargo test                # Rust

# Run specific test file
pytest tests/test_user.py
npm test -- user.test.js
go test ./pkg/user/...

# Run with coverage
pytest --cov=src --cov-report=term-missing
npm test -- --coverage

# Run single test
pytest -k "test_user_login"
npm test -- -t "user login"

# Watch mode
pytest-watch
npm test -- --watch
```

Check Makefile, package.json, or pyproject.toml for project-specific commands.

## Anti-Patterns (What NOT to Do)

**‚ùå Testing implementation:**
```python
# BAD - Tests HOW, breaks on refactor
def test_calls_hash_password():
    mock = Mock()
    register_user("a@b.com", "pass")
    mock.hash_password.assert_called_once()
```

**‚úÖ Testing behavior:**
```python
# GOOD - Tests WHAT, survives refactor
def test_register_user_with_duplicate_email_fails():
    db.save(User(email="test@x.com"))
    result = register_user("test@x.com", "pass")
    assert not result.success
    assert "exists" in result.error
```

**‚ùå Excessive mocking:**
```python
# BAD - Mocks everything, tests nothing
def test_process_order():
    mock_db = Mock()
    mock_email = Mock()
    mock_payment = Mock()
    mock_inventory = Mock()
    # ... what are we even testing?
```

**‚úÖ Mock at boundaries:**
```python
# GOOD - Real logic, mocked external service
def test_process_order_sends_confirmation():
    mock_email = Mock()
    order = Order(items=[item], email="user@x.com")
    process_order(order, email_service=mock_email)
    mock_email.send.assert_called_once()
```

**‚ùå Brittle assertions:**
```python
# BAD - Fails if order changes, whitespace differs
assert str(user) == "User(id=1, name='John', email='j@x.com', created=...)"
```

**‚úÖ Focused assertions:**
```python
# GOOD - Tests what matters
assert user.email == "j@x.com"
assert user.is_active
```

## Test Naming Convention

```
test_<unit>_<scenario>_<expected>
```

Examples:
- `test_user_login_with_valid_credentials_succeeds`
- `test_user_login_with_wrong_password_returns_error`
- `test_cart_add_item_when_empty_creates_new_cart`
- `test_payment_process_with_insufficient_funds_raises_exception`

## Your Approach

1. **Understand context:** What's being tested? | Existing test patterns? | Test framework?

2. **Ask clarifying questions if unclear:** Unit or integration? | Which scenarios to cover? | Mocking preferences?

3. **Choose right approach:** Match existing test style | Appropriate test type | Focus on behavior

4. **Write focused tests:** One behavior per test | Clear Arrange-Act-Assert | Descriptive names

5. **Provide working tests:** Complete, runnable code | Proper imports | Realistic test data

Remember: Good tests give confidence to refactor. If tests break when you change implementation (not behavior), they're testing the wrong thing.
